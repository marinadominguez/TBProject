{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from aicsimageio import AICSImage\n",
    "import napari\n",
    "from aicsimageio.readers import CziReader\n",
    "import cv2 as cv\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the image\n",
    "reader = CziReader(\"extern_Synlab_2156_17_3_MTB.czi\")\n",
    "# Get whole image\n",
    "smear = reader.get_image_data(\"MYX\", C=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048, 1504)\n",
      "10519\n",
      "98\n"
     ]
    }
   ],
   "source": [
    "# save in a new variable the information regarding the 673th tile out of 1345\n",
    "img = smear[673]\n",
    "img.shape\n",
    "\n",
    "print(img.shape)\n",
    "print(np.max(img))\n",
    "print(np.min(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class visualization:\n",
    "    def __init__(self, img):\n",
    "        self.img = img\n",
    "\n",
    "    # visualize the image with napari using its numpy array\n",
    "    def visualize_napari(self):\n",
    "        \"\"\"\n",
    "        :param numpy_img: image to be visualized\n",
    "        \"\"\"\n",
    "        with napari.gui_qt():\n",
    "            viewer = napari.Viewer()\n",
    "            viewer.add_image(self.img)\n",
    "\n",
    "    # visualize different images in the same moment, a list of images is passed as argument and we visualize them all\n",
    "    def visualize_all_list_napari(self, numpy_img_list: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param numpy_img_list: list containing different images to be visualized\n",
    "        \"\"\"\n",
    "        with napari.gui_qt():\n",
    "            viewer = napari.Viewer()\n",
    "            for img in numpy_img_list:\n",
    "                viewer.add_image(img)\n",
    "    \n",
    "    # plot histogram of pixel intensity\n",
    "    def plot_histogram(self):\n",
    "        plt.hist(self.img.ravel(), self.img.max(), [0, self.img.max()])\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class thresholding:\n",
    "    def __init__(self, img):\n",
    "        self.img = img\n",
    "\n",
    "    def otsu_thresholding(self):\n",
    "        \"\"\"\n",
    "        Threshold and binarize an image using Otsu's method\n",
    "\n",
    "        :param image: image you want to threshold\n",
    "        :return: ret: the computed threshold value\n",
    "                th: binary image (image with the threshold applied, pixels above threshold are white = 255, pixels below threshold are black= 0)\n",
    "        \"\"\"\n",
    "        ret,th = cv.threshold(self.img, 0, 255, cv.THRESH_BINARY+cv.THRESH_OTSU)\n",
    "        return ret, th\n",
    "\n",
    "    def hard_thresholding(self, threshold : int):\n",
    "        \"\"\"\n",
    "        Implement hard threshold (a threshold manually imputed). \n",
    "        Take everything above \"threshold\" to be white and everything below \"threshold\" to be black.\n",
    "\n",
    "        :param image: image to be thresholded\n",
    "        :param threshold: hard threshold to be implemented\n",
    "        :return: ret: threshold value\n",
    "                th: binary image (pixels above threshold are white = 255, pixels below threshold are black= 0)\n",
    "        \"\"\"\n",
    "        ret,th = cv.threshold(self.img, threshold, 255, cv.THRESH_BINARY)\n",
    "        return ret, th\n",
    "\n",
    "    def adaptive_thresholding(self, block_size : int, c : int):\n",
    "        \"\"\"\n",
    "        Apply adaptive thresholding to the image\n",
    "\n",
    "        :param image: image to be thresholded\n",
    "        :param block_size: size of the block used to compute the threshold\n",
    "        :param c: constant subtracted from the mean or weighted mean\n",
    "        :return: th: binary image (pixels above threshold are white = 255, pixels below threshold are black= 0)\n",
    "        \"\"\"\n",
    "        th = cv.adaptiveThreshold(self.img, 255, cv.ADAPTIVE_THRESH_MEAN_C, cv.THRESH_BINARY, block_size, c)\n",
    "        return th\n",
    "\n",
    "    def gaussian_thresholding(self, block_size : int, c : int):\n",
    "        \"\"\"\n",
    "        Apply gaussian thresholding to the image\n",
    "\n",
    "        :param image: image to be thresholded\n",
    "        :param block_size: size of the block used to compute the threshold\n",
    "        :param c: constant subtracted from the mean or weighted mean\n",
    "        :return: th: binary image (pixels above threshold are white = 255, pixels below threshold are black= 0)\n",
    "        \"\"\"\n",
    "        th = cv.adaptiveThreshold(self.img, 255, cv.ADAPTIVE_THRESH_GAUSSIAN_C, cv.THRESH_BINARY, block_size, c)\n",
    "        return th\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessing: \n",
    "    def __init__(self, img):\n",
    "        self.img = img\n",
    "\n",
    "    def sharpen(self: np.ndarray):\n",
    "        \"\"\"\n",
    "        :param image: image to be sharpened\n",
    "        :return: sharp image\n",
    "        \"\"\"\n",
    "        kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]])\n",
    "        return cv.filter2D(self, -1, kernel)\n",
    "    \n",
    "    # approach in which we sharp (in stead of blur as the example) the image before applying the thresholding\n",
    "    # sharpen the image using a high-pass filter TODO: can we do this better? sharp out better maybe in sub-images?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UTILS \n",
    "\n",
    "# approach for going through the different tiles and applying the thresholding separately\n",
    "# split the whole images into tiles\n",
    "# apply thresholding to each tile\n",
    "def split_into_tiles(self, tile_size: int):\n",
    "    \"\"\"\n",
    "    split image into tiles of shape tile_size*tile_size\n",
    "\n",
    "    :param image: image to be split\n",
    "    :param tile_size: dimensions of single tiles\n",
    "    :return: tiles: list with the different tiles\n",
    "    \"\"\"\n",
    "    tiles = []\n",
    "    for i in range(0, self.img.shape[0], tile_size):\n",
    "        for j in range(0, self.img.shape[1], tile_size):\n",
    "            tile = self.img[i:i+tile_size, j:j+tile_size]\n",
    "            tiles.append(tile)\n",
    "    return tiles\n",
    "\n",
    "\n",
    "#reconstruct image from different tiles given the number of tiles in x and y direction and a list of tiles\n",
    "def reconstruct_image(tiles: list, x_tiles: int, y_tiles: int):\n",
    "    \"\"\"\n",
    "    :param tiles:    list with the different single tiles\n",
    "    :param x_tiles:  how many tiles fit in the x axis\n",
    "    :param y_tiles:  how many tiles fit in the y axis\n",
    "    :return:         numpy array, reconstructed image\n",
    "    \"\"\"\n",
    "    big_image = np.zeros((x_tiles*tiles[0].shape[0], y_tiles*tiles[0].shape[1]))\n",
    "    for i in range(x_tiles):\n",
    "        for j in range(y_tiles):\n",
    "            big_image[i*tiles[0].shape[0]:(i+1)*tiles[0].shape[0], j*tiles[0].shape[1]:(j+1)*tiles[0].shape[1]] = tiles[i*y_tiles+j]\n",
    "    return big_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class box_creation: \n",
    "    def __init__(self, img):\n",
    "        self.img = img\n",
    "    \n",
    "\n",
    "    #connectedComponentsWithStats works better, can get centroid, in this way can put the box/rectangle around the bacilli\n",
    "    #but not all conncected components are identified, might be a problem of the image we give him\n",
    "    def get_connected_components_coordinate(self):\n",
    "        connectivity = 8\n",
    "        #find connected components\n",
    "        num_labels, labels_im, stats, centroids = cv.connectedComponentsWithStats(self.img, connectivity)\n",
    "        #get coordinates of connected components\n",
    "        coordinates = np.zeros((num_labels, 2),dtype=np.uint64)  #NO uint8\n",
    "\n",
    "        for i in range(1, num_labels):\n",
    "            coordinates[i,0] = centroids[i,0]\n",
    "            coordinates[i,1] = centroids[i,1]\n",
    "        print(coordinates)\n",
    "        return coordinates\n",
    "\n",
    "    # add the 2d bounding boxes to the image\n",
    "    def add_bounding_boxes(self, coordinates):\n",
    "        \"\"\"\n",
    "        Add whhite rectangles around bacilli\n",
    "\n",
    "        :param image: image with bacilli to be boxed\n",
    "        :param coordinates:  coordinates of the center of the bacillus\n",
    "        \"\"\"\n",
    "        for i in range(len(coordinates)):\n",
    "                x_min = coordinates[i][0]\n",
    "                x_max = coordinates[i][0]\n",
    "                y_min = coordinates[i][1]\n",
    "                y_max = coordinates[i][1]\n",
    "                cv.rectangle(self.img, (y_min+15, x_min+15), (y_max-15, x_max-15), (255, 0, 0), 4)\n",
    "\n",
    "\n",
    "    #find connected components with specified size with opencv using connectedComponentsWithStats\n",
    "    #does not seem to work very good, maybe we do it by hand...\n",
    "    #could also try with length\n",
    "    def get_connected_components_with_minimum_and_max_size(self, min_size: int,max_size: int):\n",
    "        \"\"\"\n",
    "        :param img: image where we want to find connected components. black and white immage with\n",
    "                    int8 rappr.\n",
    "        :param min_size: minimum size of the cc\n",
    "        :param max_size: maximum size of the cc\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        #connecctivity shoud tell us about how connected to they have to be to be considered as one, seems to be working poorly\n",
    "        connectivity = 8\n",
    "        #find connected components\n",
    "        num_labels, labels_im, stats, centroids = cv.connectedComponentsWithStats(self.img, connectivity)\n",
    "        #get coordinates of connected components\n",
    "        coordinates =np.array([[0,0]],dtype=np.uint64)\n",
    "        print(centroids.shape)\n",
    "        for i in range(1, num_labels):\n",
    "            if stats[i,4] > min_size and stats[i,4]< max_size:\n",
    "                coordinates=np.append(coordinates,[centroids[i,:]], axis=0)\n",
    "        coordinates=coordinates[1:,:]\n",
    "        return coordinates\n",
    "\n",
    "    def find_coordinates(image: np.ndarray):  #old version\n",
    "        \"\"\"\n",
    "        Find the coordinates of the connected compenents of the image.\n",
    "        to be more specific, find first coordinate of a component\n",
    "\n",
    "        :param image: image where we want to find the connected components\n",
    "        :return: coordinates: array with coordinates of the components in the rows\n",
    "        \"\"\"\n",
    "        num_conn_comp, labels_conn_comp = cv.connectedComponents(image)\n",
    "        #num_conn_comp tells us how many components we have\n",
    "        #labels_conn_comp is an image where the entries where there should be\n",
    "        # a component are the label of that component\n",
    "        # overwritten in such a way that the the values of the pixels are the value of the corresponding connected component.\n",
    "        labels_conn_comp = labels_conn_comp.astype(np.uint64)    #cannot use uint8 cannot rappresent all numbers with uint8\n",
    "\n",
    "        #save coordinates in two dimensional array\n",
    "        coordinates = np.zeros((num_conn_comp,2),dtype=\"int64\")\n",
    "\n",
    "        # iterate over the connected components and add the coordinates of the pixels to the list of coordinates\n",
    "        for i in range(1, num_conn_comp):\n",
    "            coordinates[i,0] = np.where(labels_conn_comp == i)[0][0]\n",
    "            coordinates[i,1] = np.where(labels_conn_comp == i)[1][0]\n",
    "        return coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class postprocessing: \n",
    "    def __init__(self, img):\n",
    "        self.img = img\n",
    "    \n",
    "    #if a black pixel is surrounded by white pixels left right up and down we set it to white, for every pixel in the image\n",
    "    #to help the connected component function, maybe we can improve this\n",
    "    def remove_black_pixels_in_white(self):\n",
    "        \"\"\"\n",
    "        :param img: image to be better\n",
    "        :return:    better immage with less black holes in white parts\n",
    "        \"\"\"\n",
    "        #what is white? 255 or what?\n",
    "        max=self.img.max()\n",
    "        #init return image\n",
    "        ret_img=np.zeros((self.img.shape[0], self.img.shape[1]))\n",
    "\n",
    "        for i in range(1,self.img.shape[0]-1):\n",
    "            for j in range(1, self.img.shape[1]-1):\n",
    "                if self.img[i,j] == 0:\n",
    "                    if self.img[i-1,j] > 0 or self.img[i+1,j] > 0 or self.img[i,j-1] > 0 or self.img[i,j+1] > 0:\n",
    "                        ret_img[i,j] = max\n",
    "                    else:\n",
    "                        ret_img[i,j]=self.img[i,j]\n",
    "        return ret_img\n",
    "    \n",
    "    #check if image is to be set to 0, if we have more than 400 pixels with value 0 or less then 600 pixels with value 0 we set the image to 0\n",
    "    def check_image(self):\n",
    "        \"\"\"\n",
    "        For every sub-image we check if its worth keeping or not\n",
    "\n",
    "        :param img: image to be checked\n",
    "        :return: bool\n",
    "        \"\"\"\n",
    "        if np.sum(self.img == 0) > 200 and np.sum(self.img == 0) < 800: #maybe check for >0\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    #set pixels that are 255 to zero (black)\n",
    "    def set_zero(img):\n",
    "        h=img\n",
    "        h[h>0]=0\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hard_thresholding_shit_boxes(img: np.ndarray):\n",
    "    \"\"\"\n",
    "    Implement tresholding with a hard treshold (10000).\n",
    "    Find components and box coordinates, draw the boxes with\n",
    "    first coordinate of the cc.\n",
    "\n",
    "    :param img: image to be tresholded\n",
    "    \"\"\"\n",
    "    #sharpen image\n",
    "    sharpened_img = preprocessing.sharpen(img)\n",
    "    #hard threshold the sharpened image 10000\n",
    "    threshold, hard_threshold = thresholding.hard_thresholding(sharpened_img, 10000)\n",
    "    #convert to uint8 s. t. coonected  component function can accept it\n",
    "    hard_threshold=np.uint8(hard_threshold)\n",
    "    #find coordinates of connected components, first coordinate, shit because not centered\n",
    "    coordinates= box_creation.find_coordinates(hard_threshold)\n",
    "    #add bounding boxes to the image\n",
    "    box_creation.add_bounding_boxes(hard_threshold,coordinates)\n",
    "    #visualize\n",
    "    visualization.visualize_all_list_napari([hard_threshold,sharpened_img,img])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def otsu_split_thresholding(img: np.ndarray, tile_size=32):\n",
    "    \"\"\"\n",
    "    Perform Otsu tresholding on sub images of 32 x 32,\n",
    "    then reconstruct\n",
    "\n",
    "    :param img:  image to be tresholded\n",
    "    \"\"\"\n",
    "    #sharpen image\n",
    "    sharpened_img=preprocessing.sharpen(img)\n",
    "    #split\n",
    "    tiles=split_into_tiles(sharpened_img,tile_size)\n",
    "    #otsu on big image\n",
    "    r_big,th_big=thresholding.otsu_thresholding(sharpened_img)\n",
    "    #otsu on sub-images\n",
    "    thresholded_tiles=[]\n",
    "    for t in tiles:\n",
    "        r,th=thresholding.otsu_thresholding(t)\n",
    "        thresholded_tiles.append(th)\n",
    "    #reconstruct\n",
    "    reconstructed_image=reconstruct_image(thresholded_tiles,64,47)\n",
    "    #visualize\n",
    "    visualization.visualize_all_list_napari([reconstructed_image,sharpened_img,img,th_big])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def otsu_cleaned_black_split_thresholding(img):\n",
    "    \"\"\"\n",
    "    Performed otsu on an image, then cleaned the image by setting tiles with no basilli to black\n",
    "\n",
    "    :param img: image to be tresholded\n",
    "    :return:    tresholded clean image\n",
    "    \"\"\"\n",
    "\n",
    "    sharpened_img= preprocessing.sharpen(img)\n",
    "\n",
    "    tiles=split_into_tiles(sharpened_img,32)\n",
    "\n",
    "    thresholded_tiles=[]\n",
    "    for t in tiles:\n",
    "        r,th_image=thresholding.otsu_thresholding(t)\n",
    "        thresholded_tiles.append(th_image)\n",
    "    reconstructed_image=reconstruct_image(thresholded_tiles,64,47)\n",
    "\n",
    "    cleaned_tiles=[]\n",
    "    for tl in thresholded_tiles:\n",
    "           if postprocessing.check_image(tl):\n",
    "                m=postprocessing.set_zero(tl)                      #clean in black only thing that changes\n",
    "                cleaned_tiles.append(m)\n",
    "           else:\n",
    "               cleaned_tiles.append(tl)\n",
    "\n",
    "    reconstructed_clean_image=reconstruct_image(cleaned_tiles,64,47)\n",
    "\n",
    "    visualization.visualize_all_list_napari([reconstructed_image,reconstructed_clean_image,sharpened_img,img])\n",
    "\n",
    "    return reconstructed_clean_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "cleaned_im_black=postprocessing.remove_black_pixels_in_white(cleaned_im_black)\n",
    "cleaned_im_black_int=np.unit8(cleaned_im_black)\n",
    "\n",
    "new_coordinates=box_creation.get_connected_components_with_minimum_and_max_size(cleaned_im_black_int, 0,100000)\n",
    "\n",
    "\n",
    "new_coordinates=new_coordinates.astype(int)\n",
    "\n",
    "box_creation.add_bounding_boxes(cleaned_im_black,new_coordinates)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "images = [img, cleaned_im_black]\n",
    "\n",
    "visualization.visualize_all_list_napari(images)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a65a227fba0061ae988ce10ff56f3113fbb17fb66baceb7b93216863516eea90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
